"""
ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«ä¿å­˜ãƒ»æ¤œç´¢ã™ã‚‹ãŸã‚ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import logging
import json
import pickle
from pathlib import Path
from typing import Dict, List, Optional, Tuple
from datetime import datetime
import hashlib

import openai
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
try:
    from langchain_openai import OpenAIEmbeddings
except ImportError:
    from langchain_community.embeddings import OpenAIEmbeddings
from langchain_core.documents import Document

from .config import Config

class VectorStore:
    """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Config):
        """
        åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # OpenAI APIã‚­ãƒ¼ã¯æ–°ã—ã„ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆå½¢å¼ã§ä½¿ç”¨
        
        # åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            model=self.config.rag.embedding_model,
            api_key=self.config.openai.api_key
        )
        
        # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²å™¨ã®åˆæœŸåŒ–
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.rag.chunk_size,
            chunk_overlap=self.config.rag.chunk_overlap,
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ã€", " ", ""]
        )
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ãƒ‘ã‚¹
        self.vectorstore_path = self.config.get_storage_path('vectorstore')
        self.vectorstore_path.mkdir(parents=True, exist_ok=True)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        self.metadata_file = self.vectorstore_path / "metadata.json"
        
        # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        self.vectorstore = None
        self.metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict:
        """
        ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®è¾æ›¸
        """
        try:
            if self.metadata_file.exists():
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            else:
                return {
                    'created_at': datetime.now().isoformat(),
                    'last_updated': None,
                    'document_count': 0,
                    'video_count': 0,
                    'videos': {}
                }
        except Exception as e:
            self.logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return {
                'created_at': datetime.now().isoformat(),
                'last_updated': None,
                'document_count': 0,
                'video_count': 0,
                'videos': {}
            }
    
    def _save_metadata(self):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ã«å¤±æ•—: {e}")
    
    def load(self) -> bool:
        """
        æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            èª­ã¿è¾¼ã¿ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            faiss_index_path = self.vectorstore_path / "index.faiss"
            faiss_pkl_path = self.vectorstore_path / "index.pkl"
            
            if faiss_index_path.exists() and faiss_pkl_path.exists():
                try:
                    # æ–°ã—ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®ãƒ­ãƒ¼ãƒ‰
                    self.vectorstore = FAISS.load_local(
                        str(self.vectorstore_path), 
                        self.embeddings,
                        allow_dangerous_deserialization=True
                    )
                except TypeError:
                    # å¤ã„ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã§ã®ãƒ­ãƒ¼ãƒ‰
                    self.vectorstore = FAISS.load_local(
                        str(self.vectorstore_path), 
                        self.embeddings
                    )
                self.logger.info("æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                return True
            else:
                self.logger.info("æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return False
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return False
    
    def add_transcript(self, transcript_data: Dict) -> bool:
        """
        å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
        
        Args:
            transcript_data: å­—å¹•ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            è¿½åŠ ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            video_id = transcript_data['video_id']
            
            # æ—¢ã«è¿½åŠ æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
            if video_id in self.metadata['videos']:
                self.logger.info(f"å‹•ç”»ã¯æ—¢ã«ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ æ¸ˆã¿: {video_id}")
                return True
            
            # å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
            transcript_text = self._combine_transcript_text(transcript_data['transcript'])
            
            if not transcript_text.strip():
                self.logger.warning(f"å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆãŒç©ºã§ã™: {video_id}")
                return False
            
            # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
            documents = self._create_documents(transcript_data, transcript_text)
            
            if not documents:
                self.logger.warning(f"ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ: {video_id}")
                return False
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ 
            if self.vectorstore is None:
                # æ–°è¦ä½œæˆ
                self.vectorstore = FAISS.from_documents(documents, self.embeddings)
                self.logger.info("æ–°è¦ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸ")
            else:
                # æ—¢å­˜ã«è¿½åŠ 
                self.vectorstore.add_documents(documents)
                self.logger.info(f"æ—¢å­˜ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ ã—ã¾ã—ãŸ: {len(documents)} ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ")
            
            # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜
            self._save_vectorstore()
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°
            self.metadata['videos'][video_id] = {
                'title': transcript_data['title'],
                'uploader': transcript_data['uploader'],
                'url': transcript_data['url'],
                'added_at': datetime.now().isoformat(),
                'document_count': len(documents),
                'duration': transcript_data.get('duration', 0)
            }
            self.metadata['last_updated'] = datetime.now().isoformat()
            self.metadata['document_count'] += len(documents)
            self.metadata['video_count'] = len(self.metadata['videos'])
            self._save_metadata()
            
            self.logger.info(f"å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ : {transcript_data['title']}")
            return True
            
        except Exception as e:
            self.logger.error(f"å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ã«å¤±æ•—: {e}")
            return False
    
    def _combine_transcript_text(self, transcript: List[Dict]) -> str:
        """
        å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«çµåˆ
        
        Args:
            transcript: å­—å¹•ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            çµåˆã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ
        """
        if not transcript:
            return ""
        
        text_parts = []
        for entry in transcript:
            text = entry.get('text', '').strip()
            if text:
                text_parts.append(text)
        
        return ' '.join(text_parts)
    
    def _create_documents(self, transcript_data: Dict, transcript_text: str) -> List[Document]:
        """
        å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
        
        Args:
            transcript_data: å­—å¹•ãƒ‡ãƒ¼ã‚¿
            transcript_text: å­—å¹•ãƒ†ã‚­ã‚¹ãƒˆ
            
        Returns:
            ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®ãƒªã‚¹ãƒˆ
        """
        # ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²
        chunks = self.text_splitter.split_text(transcript_text)
        
        documents = []
        for i, chunk in enumerate(chunks):
            # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’è¨ˆç®—ï¼ˆæ¦‚ç®—ï¼‰
            chunk_position = i / len(chunks)
            estimated_timestamp = int(transcript_data.get('duration', 0) * chunk_position)
            
            # YouTubeã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—URLã‚’ç”Ÿæˆ
            timestamp_url = f"{transcript_data['url']}&t={estimated_timestamp}s"
            
            metadata = {
                'video_id': transcript_data['video_id'],
                'title': transcript_data['title'],
                'uploader': transcript_data['uploader'],
                'url': transcript_data['url'],
                'timestamp_url': timestamp_url,
                'timestamp': estimated_timestamp,
                'chunk_id': i,
                'total_chunks': len(chunks),
                'upload_date': transcript_data.get('upload_date', ''),
                'duration': transcript_data.get('duration', 0),
                'view_count': transcript_data.get('view_count', 0),
                'channel_id': transcript_data.get('channel_id', ''),
                'channel_url': transcript_data.get('channel_url', ''),
            }
            
            documents.append(Document(
                page_content=chunk,
                metadata=metadata
            ))
        
        return documents
    
    def _save_vectorstore(self):
        """ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜"""
        try:
            if self.vectorstore:
                self.vectorstore.save_local(str(self.vectorstore_path))
                self.logger.debug("ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®ä¿å­˜ã«å¤±æ•—: {e}")
    
    def search(self, query: str, k: int = None) -> List[Dict]:
        """
        ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã§æ¤œç´¢
        
        Args:
            query: æ¤œç´¢ã‚¯ã‚¨ãƒª
            k: å–å¾—ã™ã‚‹çµæœæ•°
            
        Returns:
            æ¤œç´¢çµæœã®ãƒªã‚¹ãƒˆ
        """
        if not self.vectorstore:
            self.logger.warning("ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“")
            return []
        
        if k is None:
            k = self.config.rag.retrieval_k
        
        try:
            # é¡ä¼¼åº¦æ¤œç´¢
            docs_with_scores = self.vectorstore.similarity_search_with_score(query, k=k)
            
            results = []
            for doc, score in docs_with_scores:
                # é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢ã‚’0-1ã®ç¯„å›²ã«æ­£è¦åŒ–ï¼ˆFAISSã®è·é›¢ã¯å°ã•ã„ã»ã©é¡ä¼¼ï¼‰
                similarity_score = 1 / (1 + score)
                
                # é–¾å€¤ãƒã‚§ãƒƒã‚¯
                if similarity_score < self.config.rag.similarity_threshold:
                    continue
                
                result = {
                    'content': doc.page_content,
                    'score': similarity_score,
                    'video_id': doc.metadata['video_id'],
                    'title': doc.metadata['title'],
                    'uploader': doc.metadata['uploader'],
                    'url': doc.metadata['url'],
                    'timestamp_url': doc.metadata['timestamp_url'],
                    'timestamp': doc.metadata['timestamp'],
                    'chunk_id': doc.metadata['chunk_id'],
                    'upload_date': doc.metadata.get('upload_date', ''),
                    'duration': doc.metadata.get('duration', 0),
                    'view_count': doc.metadata.get('view_count', 0),
                }
                results.append(result)
            
            # ã‚¹ã‚³ã‚¢ã®é«˜ã„é †ã«ã‚½ãƒ¼ãƒˆ
            results.sort(key=lambda x: x['score'], reverse=True)
            
            self.logger.info(f"æ¤œç´¢å®Œäº†: {len(results)} ä»¶ã®çµæœ")
            return results
            
        except Exception as e:
            self.logger.error(f"æ¤œç´¢ã«å¤±æ•—: {e}")
            return []
    
    def update_from_file(self, file_path: Path) -> bool:
        """
        æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’æ›´æ–°
        
        Args:
            file_path: æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Returns:
            æ›´æ–°ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            # ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å­—å¹•ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿
            with open(file_path, 'r', encoding='utf-8') as f:
                transcript_data = json.load(f)
            
            video_id = transcript_data['video_id']
            
            # æ—¢å­˜ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‰Šé™¤ï¼ˆFAISSã§ã¯é›£ã—ã„ã®ã§å†æ§‹ç¯‰ï¼‰
            if video_id in self.metadata['videos']:
                self.logger.info(f"å‹•ç”»ã®æ›´æ–°ã®ãŸã‚å†æ§‹ç¯‰ãŒå¿…è¦: {video_id}")
                # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã€FAISSã®åˆ¶é™ã«ã‚ˆã‚Šå®Œå…¨ãªå†æ§‹ç¯‰ãŒå¿…è¦
                # ã“ã“ã§ã¯ç°¡æ˜“çš„ã«è¿½åŠ ã®ã¿å®Ÿè£…
                return self.add_transcript(transcript_data)
            else:
                return self.add_transcript(transcript_data)
            
        except Exception as e:
            self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã®æ›´æ–°ã«å¤±æ•—: {file_path} - {e}")
            return False
    
    def get_statistics(self) -> Dict:
        """
        ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        return {
            'video_count': self.metadata['video_count'],
            'document_count': self.metadata['document_count'],
            'last_updated': self.metadata['last_updated'],
            'created_at': self.metadata['created_at'],
            'videos': list(self.metadata['videos'].keys())
        }
    
    def remove_video(self, video_id: str) -> bool:
        """
        å‹•ç”»ã‚’ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‹ã‚‰å‰Šé™¤
        
        Args:
            video_id: å‰Šé™¤ã™ã‚‹å‹•ç”»ã®ID
            
        Returns:
            å‰Šé™¤ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        # FAISSã§ã¯å€‹åˆ¥ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆå‰Šé™¤ãŒå›°é›£
        # å®Ÿè£…ãŒå¿…è¦ãªå ´åˆã¯ã€å…¨ä½“ã‚’å†æ§‹ç¯‰ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        self.logger.warning("FAISSã§ã¯å€‹åˆ¥å‰Šé™¤ã¯å›°é›£ã§ã™ã€‚å†æ§‹ç¯‰ãŒå¿…è¦ã§ã™ã€‚")
        return False
    
    def rebuild_from_transcripts(self, transcripts_dir: Path) -> bool:
        """
        å­—å¹•ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã‚’å†æ§‹ç¯‰
        
        Args:
            transcripts_dir: å­—å¹•ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
            
        Returns:
            å†æ§‹ç¯‰ã«æˆåŠŸã—ãŸå ´åˆTrue
        """
        try:
            self.logger.info("ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å†æ§‹ç¯‰ã‚’é–‹å§‹")
            
            # æ—¢å­˜ã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã¨ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¯ãƒªã‚¢
            self.vectorstore = None
            self.metadata = {
                'created_at': datetime.now().isoformat(),
                'last_updated': None,
                'document_count': 0,
                'video_count': 0,
                'videos': {}
            }
            
            # å­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§è¿½åŠ 
            transcript_files = list(transcripts_dir.glob("*.json"))
            success_count = 0
            
            for file_path in transcript_files:
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        transcript_data = json.load(f)
                    
                    if self.add_transcript(transcript_data):
                        success_count += 1
                        self.logger.info(f"å†æ§‹ç¯‰: {transcript_data['title']}")
                    
                except Exception as e:
                    self.logger.error(f"ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã‚¨ãƒ©ãƒ¼: {file_path} - {e}")
                    continue
            
            self.logger.info(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å†æ§‹ç¯‰å®Œäº†: {success_count}/{len(transcript_files)} ä»¶")
            return success_count > 0
            
        except Exception as e:
            self.logger.error(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã®å†æ§‹ç¯‰ã«å¤±æ•—: {e}")
            return False
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.get_statistics()
        print("="*50)
        print("ğŸ—„ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢çµ±è¨ˆ")
        print("="*50)
        print(f"ğŸ¬ å‹•ç”»æ•°: {stats['video_count']} ä»¶")
        print(f"ğŸ“„ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°: {stats['document_count']} ä»¶")
        print(f"ğŸ•’ æœ€çµ‚æ›´æ–°: {stats['last_updated'] or 'ãªã—'}")
        print(f"ğŸ“… ä½œæˆæ—¥: {stats['created_at']}")
        print("="*50)