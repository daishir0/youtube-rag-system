"""
å‡¦ç†æ¸ˆã¿URLç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
YouTubeå‹•ç”»URLã®å‡¦ç†çŠ¶æ³ã‚’ç®¡ç†ã—ã€é‡è¤‡å‡¦ç†ã‚’é˜²ã
"""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Set
from datetime import datetime
import os

from .config import Config

class URLManager:
    """å‡¦ç†æ¸ˆã¿URLç®¡ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, config: Config):
        """
        åˆæœŸåŒ–
        
        Args:
            config: è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.processed_urls_file = self.config.get_storage_path('processed_urls')
        self.processed_urls = self._load_processed_urls()
    
    def _load_processed_urls(self) -> Dict[str, Dict]:
        """
        å‡¦ç†æ¸ˆã¿URLãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã¿
        
        Returns:
            å‡¦ç†æ¸ˆã¿URLã®è¾æ›¸
        """
        try:
            if self.processed_urls_file.exists():
                with open(self.processed_urls_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.logger.info(f"å‡¦ç†æ¸ˆã¿URL {len(data)} ä»¶ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
                    return data
            else:
                self.logger.info("å‡¦ç†æ¸ˆã¿URLãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“ã€‚æ–°è¦ä½œæˆã—ã¾ã™")
                return {}
        except Exception as e:
            self.logger.error(f"å‡¦ç†æ¸ˆã¿URLãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—: {e}")
            return {}
    
    def _save_processed_urls(self):
        """å‡¦ç†æ¸ˆã¿URLãƒªã‚¹ãƒˆã‚’ä¿å­˜"""
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½œæˆ
            self.processed_urls_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(self.processed_urls_file, 'w', encoding='utf-8') as f:
                json.dump(self.processed_urls, f, ensure_ascii=False, indent=2)
            
            self.logger.debug("å‡¦ç†æ¸ˆã¿URLãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ")
            
        except Exception as e:
            self.logger.error(f"å‡¦ç†æ¸ˆã¿URLãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜ã«å¤±æ•—: {e}")
    
    def is_processed(self, url: str) -> bool:
        """
        URLãŒå‡¦ç†æ¸ˆã¿ã‹ãƒã‚§ãƒƒã‚¯
        
        Args:
            url: ãƒã‚§ãƒƒã‚¯ã™ã‚‹URL
            
        Returns:
            å‡¦ç†æ¸ˆã¿ã®å ´åˆTrue
        """
        return url in self.processed_urls
    
    def mark_processed(self, url: str, video_data: Dict):
        """
        URLã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯
        
        Args:
            url: å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯ã™ã‚‹URL
            video_data: å‹•ç”»ãƒ‡ãƒ¼ã‚¿
        """
        self.processed_urls[url] = {
            'video_id': video_data.get('video_id'),
            'title': video_data.get('title'),
            'uploader': video_data.get('uploader'),
            'processed_date': datetime.now().isoformat(),
            'file_path': str(self.config.get_storage_path('transcripts') / f"{video_data.get('video_id')}.json"),
            'vectorstore_added': False,  # ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«è¿½åŠ ã•ã‚ŒãŸã‹ã®ãƒ•ãƒ©ã‚°
            'last_modified': datetime.now().isoformat()
        }
        self._save_processed_urls()
        self.logger.info(f"URLã‚’å‡¦ç†æ¸ˆã¿ã¨ã—ã¦ãƒãƒ¼ã‚¯: {url}")
    
    def mark_vectorstore_added(self, url: str):
        """
        URLã®ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢è¿½åŠ ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°
        
        Args:
            url: æ›´æ–°ã™ã‚‹URL
        """
        if url in self.processed_urls:
            self.processed_urls[url]['vectorstore_added'] = True
            self.processed_urls[url]['last_modified'] = datetime.now().isoformat()
            self._save_processed_urls()
            self.logger.info(f"ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢è¿½åŠ ãƒ•ãƒ©ã‚°ã‚’æ›´æ–°: {url}")
    
    def get_processed_urls(self) -> Dict[str, Dict]:
        """
        å‡¦ç†æ¸ˆã¿URLã®è¾æ›¸ã‚’å–å¾—
        
        Returns:
            å‡¦ç†æ¸ˆã¿URLã®è¾æ›¸
        """
        return self.processed_urls.copy()
    
    def get_processed_count(self) -> int:
        """
        å‡¦ç†æ¸ˆã¿URLæ•°ã‚’å–å¾—
        
        Returns:
            å‡¦ç†æ¸ˆã¿URLæ•°
        """
        return len(self.processed_urls)
    
    def get_unprocessed_for_vectorstore(self) -> List[str]:
        """
        ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«æœªè¿½åŠ ã®URLãƒªã‚¹ãƒˆã‚’å–å¾—
        
        Returns:
            ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢ã«æœªè¿½åŠ ã®URLãƒªã‚¹ãƒˆ
        """
        unprocessed = []
        for url, data in self.processed_urls.items():
            if not data.get('vectorstore_added', False):
                unprocessed.append(url)
        return unprocessed
    
    def remove_processed_url(self, url: str):
        """
        å‡¦ç†æ¸ˆã¿URLã‚’å‰Šé™¤
        
        Args:
            url: å‰Šé™¤ã™ã‚‹URL
        """
        if url in self.processed_urls:
            del self.processed_urls[url]
            self._save_processed_urls()
            self.logger.info(f"å‡¦ç†æ¸ˆã¿URLã‚’å‰Šé™¤: {url}")
    
    def find_updated_transcripts(self) -> List[Path]:
        """
        æ›´æ–°ã•ã‚ŒãŸå­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢
        
        Returns:
            æ›´æ–°ã•ã‚ŒãŸå­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ãƒªã‚¹ãƒˆ
        """
        updated_files = []
        transcripts_dir = self.config.get_storage_path('transcripts')
        
        if not transcripts_dir.exists():
            return updated_files
        
        for url, data in self.processed_urls.items():
            video_id = data.get('video_id')
            if not video_id:
                continue
            
            transcript_file = transcripts_dir / f"{video_id}.json"
            if not transcript_file.exists():
                continue
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
            file_modified_time = datetime.fromtimestamp(transcript_file.stat().st_mtime)
            
            # å‡¦ç†æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã®æœ€çµ‚æ›´æ–°æ—¥æ™‚ã‚’å–å¾—
            last_modified_str = data.get('last_modified', data.get('processed_date'))
            if last_modified_str:
                try:
                    last_modified = datetime.fromisoformat(last_modified_str)
                    
                    # ãƒ•ã‚¡ã‚¤ãƒ«ã®æ–¹ãŒæ–°ã—ã„å ´åˆã¯æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦è¿½åŠ 
                    if file_modified_time > last_modified:
                        updated_files.append(transcript_file)
                        self.logger.info(f"æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ¤œå‡º: {transcript_file}")
                except ValueError:
                    # æ—¥æ™‚ã®ãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—
                    continue
        
        return updated_files
    
    def update_file_timestamp(self, video_id: str):
        """
        ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°
        
        Args:
            video_id: å‹•ç”»ID
        """
        # è©²å½“ã™ã‚‹URLã‚’æ¤œç´¢
        for url, data in self.processed_urls.items():
            if data.get('video_id') == video_id:
                data['last_modified'] = datetime.now().isoformat()
                self._save_processed_urls()
                self.logger.info(f"ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æ›´æ–°: {video_id}")
                break
    
    def get_video_data_by_id(self, video_id: str) -> Optional[Dict]:
        """
        å‹•ç”»IDã‹ã‚‰å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        
        Args:
            video_id: å‹•ç”»ID
            
        Returns:
            å‹•ç”»ãƒ‡ãƒ¼ã‚¿ã€è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯None
        """
        for url, data in self.processed_urls.items():
            if data.get('video_id') == video_id:
                return {
                    'url': url,
                    **data
                }
        return None
    
    def get_statistics(self) -> Dict:
        """
        çµ±è¨ˆæƒ…å ±ã‚’å–å¾—
        
        Returns:
            çµ±è¨ˆæƒ…å ±ã®è¾æ›¸
        """
        total_count = len(self.processed_urls)
        vectorstore_added_count = sum(1 for data in self.processed_urls.values() 
                                     if data.get('vectorstore_added', False))
        
        return {
            'total_processed': total_count,
            'vectorstore_added': vectorstore_added_count,
            'vectorstore_pending': total_count - vectorstore_added_count,
            'updated_files': len(self.find_updated_transcripts())
        }
    
    def print_statistics(self):
        """çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º"""
        stats = self.get_statistics()
        print("="*50)
        print("ğŸ“Š URLç®¡ç†çµ±è¨ˆ")
        print("="*50)
        print(f"ğŸ“º å‡¦ç†æ¸ˆã¿å‹•ç”»: {stats['total_processed']} ä»¶")
        print(f"ğŸ—ï¸ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢è¿½åŠ æ¸ˆã¿: {stats['vectorstore_added']} ä»¶")
        print(f"â³ ãƒ™ã‚¯ã‚¿ãƒ¼ã‚¹ãƒˆã‚¢æœªè¿½åŠ : {stats['vectorstore_pending']} ä»¶")
        print(f"ğŸ”„ æ›´æ–°ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«: {stats['updated_files']} ä»¶")
        print("="*50)
    
    def cleanup_missing_files(self):
        """
        å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†æ¸ˆã¿URLã‚’å‰Šé™¤
        """
        transcripts_dir = self.config.get_storage_path('transcripts')
        urls_to_remove = []
        
        for url, data in self.processed_urls.items():
            video_id = data.get('video_id')
            if video_id:
                transcript_file = transcripts_dir / f"{video_id}.json"
                if not transcript_file.exists():
                    urls_to_remove.append(url)
        
        for url in urls_to_remove:
            self.remove_processed_url(url)
            self.logger.info(f"å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†æ¸ˆã¿URLã‚’å‰Šé™¤: {url}")
        
        if urls_to_remove:
            self.logger.info(f"ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {len(urls_to_remove)} ä»¶ã®å‡¦ç†æ¸ˆã¿URLã‚’å‰Šé™¤")
        else:
            self.logger.info("ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—: å‰Šé™¤å¯¾è±¡ãªã—")